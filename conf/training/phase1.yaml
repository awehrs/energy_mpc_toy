# Phase 1 Training Configuration: Encoder + Decoder Only
# Focus on learning stable decode patterns without interference

seed: 42
learning_rate: 1e-5
lr_scheduler_type: constant # "linear", "cosine", "constant"
logging_strategy: steps
logging_steps: 10
logging_nan_inf_filter: false
save_strategy: steps
save_steps: 500
save_total_limit: 3
eval_strategy: "no"
eval_steps: 500
eval_accumulation_steps: 
log_dir: phase1
log_wandb: true
output_dir: "data/out/phase1"
wandb_project: "training"
wandb_entity: awehrs
wandb_run: 
wandb_run_name: 
max_steps: -1
num_epochs: 3
warmup_steps: 500
gradient_checkpointing: false
gradient_accumulation_steps: 1
per_device_train_batch_size: 2
per_device_eval_batch_size: 4
weight_decay: 0.0
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1e-8
max_grad_norm: 1.0 
mixed_precision: bf16 # "none", "fp16", "bf16"
dtype: bf16
half_precision_backend: auto
verbose: false
backend: nccl
device: cuda
jit_mode_eval: false 
torch_compile: false
torch_compile_mode:
quantization:
fuse_gelu:
deepspeed:
init_retriever: true
disable_tqdm: false
resume_from_checkpoint: false
print_dataset_stats: true 
torch_empty_cache_steps: 100
dataloader_drop_last: true
prediction_loss_only: true
dataloader_num_workers: 0
dataloader_pin_memory: true

# Phase 1 specific: Per-chunk decode losses + VICReg
decode_loss_weight: 1.0  # Final step weight
intermediate_decode_weight: 0.1  # Intermediate step weight
vicreg_weight: 0.01  # VICReg regularization weight
energy_loss_weight: 0.0  # Disabled in Phase 1
forward_loss_weight: 0.0  # Disabled in Phase 1

# Optional gentle monotonicity encouragement (not needed with VICReg)
monotonic_bonus_weight: 0.0

# Mode collapse monitoring
mode_collapse_threshold: 0.01
log_detailed_metrics: true