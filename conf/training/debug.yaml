# ================================
# Energy MPC Training Debug Config
# ================================
# Optimized for local testing with small batches and frequent logging

# Basic Training Parameters
name: debug
seed: 42
num_epochs: 2
max_steps: 3  # Very short runs for debugging
per_device_train_batch_size: 2  # Small for local testing
per_device_eval_batch_size: 2
gradient_accumulation_steps: 2  # Still get effective batch size of 4

# Learning Rate & Optimization
learning_rate: 1e-4
lr_scheduler_type: "linear"  # Simple scheduler for debugging
warmup_steps: 2
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1e-8
weight_decay: 0.01
max_grad_norm: 1.0

# Mixed Precision & Compilation (keep simple for debugging)
mixed_precision: "no"  # Avoid fp16/bf16 issues during debugging
torch_compile: false
torch_compile_mode: "default"

# Logging & Evaluation (frequent for debugging)
logging_steps: 1
eval_steps: 3
save_steps: 5
save_total_limit: 2

# Output & Checkpointing
output_dir: "outputs/debug_run"
resume_from_checkpoint: null

# Wandb Configuration
log_wandb: true
wandb_project: "energy_mpc_debug"
wandb_entity: null
wandb_run_name: null

# ================================
# Energy MPC Specific Parameters
# ================================

# Sequence Plot Logging
plot_logging_frequency: 10  # Plot CE vs Energy frequently

# Loss Component Weights
w_decode: 1.0      # Decoding loss weight
w_energy: 0.5      # Energy supervision weight
w_mono: 0.2        # Monotonicity penalty weight
w_forward: 1.0     # Forward prediction loss weight
mono_margin: 0.0   # Monotonicity margin (start with 0)

# ================================
# Dataloader & System Settings
# ================================

# Dataloader (keep simple for debugging)
dataloader_num_workers: 0  # Single-threaded
dataloader_pin_memory: false
dataloader_drop_last: true

# ================================
# Dataset Building Configuration
# ================================

# Dataset building happens automatically based on file detection

# Raw data and preprocessing
raw_data_path: "data/hotpotqa/hotpot_train_v1.1_debug100.json"
preprocessing:
  model_name: "gpt2"
  chunk_size: 256
  overlap_size: 32

# Index building
index_building:
  model_name: "sentence-transformers/all-MiniLM-L6-v2"
  index_type: "flat"
  batch_size: 32
  device: "cpu"

# Dataset Paths (generated automatically if build_dataset=true)
chunks_file: null
examples_file: null
index_dir: null
index_name: null

# Optional Evaluation Dataset
eval_chunks_file: null
eval_examples_file: null
eval_index_dir: null
eval_index_name: null





    

  
  


  

